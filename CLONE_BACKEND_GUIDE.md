# ğŸ“‹ GuÃ­a para Clonar Frontend y Cambiar Backend

Esta guÃ­a te permite clonar la **Concordancia BÃ­blica Inteligente** manteniendo todo el frontend (UI/UX) pero cambiando el backend (de OpenAI Assistant API a RAG Gemini + n8n u otro).

---

## ğŸ¯ Objetivo

Mantener **100% del frontend** (pantalla de bienvenida, conversation starters, UI, animaciones, etc.) y reemplazar solo el **backend** (`api/chat.js`) para usar:
- RAG con Gemini
- OrquestaciÃ³n con n8n
- O cualquier otro servicio de IA

---

## ğŸ“ Estructura del Proyecto Actual

```
rag-openai-chatbot/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ chat.js              # âš ï¸ BACKEND - Cambiar este archivo
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html           # âœ… FRONTEND - Mantener sin cambios
â”œâ”€â”€ .env                     # ConfiguraciÃ³n de API keys
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ vercel.json
â””â”€â”€ README.md
```

---

## ğŸš€ Pasos para Clonar y Modificar

### **Paso 1: Clonar el Repositorio**

```bash
# OpciÃ³n A: Clonar desde GitHub (recomendado)
git clone https://github.com/planckc/rag-openai-chatbot-biblical-assistant.git mi-nuevo-proyecto
cd mi-nuevo-proyecto

# OpciÃ³n B: Copiar la carpeta localmente
cp -r rag-openai-chatbot mi-nuevo-proyecto
cd mi-nuevo-proyecto
```

### **Paso 2: Cambiar el Repositorio Remoto**

```bash
# Eliminar el remote del proyecto original
git remote remove origin

# Crear un nuevo repositorio en GitHub (ve a github.com/new)
# Luego conecta tu proyecto al nuevo repo:
git remote add origin https://github.com/TU_USUARIO/TU_NUEVO_REPO.git
git branch -M main
git push -u origin main
```

### **Paso 3: Instalar Dependencias**

```bash
npm install
```

---

## ğŸ”§ Modificar el Backend

### **Archivo a Modificar: `api/chat.js`**

Este es el **ÃšNICO archivo del backend** que necesitas cambiar. El frontend (`public/index.html`) se mantiene **SIN CAMBIOS**.

#### **Contrato del Backend (API Contract)**

El frontend espera que `api/chat.js` cumpla con este contrato:

**Endpoint:**
- `POST /api/chat`

**Request Body:**
```json
{
  "message": "Pregunta del usuario",
  "threadId": "opcional-id-de-conversacion"
}
```

**Response (Server-Sent Events):**

El backend debe enviar eventos SSE (Server-Sent Events) en este formato:

```javascript
// 1. Enviar Thread ID (para mantener conversaciÃ³n)
res.write(`data: ${JSON.stringify({ type: 'thread', threadId: 'abc123' })}\n\n`);

// 2. Iniciar respuesta (opcional)
res.write(`data: ${JSON.stringify({ type: 'start' })}\n\n`);

// 3. Enviar texto en chunks (streaming)
res.write(`data: ${JSON.stringify({ type: 'text', content: 'Hola ' })}\n\n`);
res.write(`data: ${JSON.stringify({ type: 'text', content: 'mundo' })}\n\n`);

// 4. Finalizar stream
res.write(`data: ${JSON.stringify({ type: 'done' })}\n\n`);
res.end();

// En caso de error:
res.write(`data: ${JSON.stringify({ type: 'error', error: 'Mensaje de error' })}\n\n`);
res.end();
```

**Headers requeridos:**
```javascript
res.setHeader('Content-Type', 'text/event-stream');
res.setHeader('Cache-Control', 'no-cache, no-transform');
res.setHeader('Connection', 'keep-alive');
res.setHeader('X-Accel-Buffering', 'no');
```

---

## ğŸ“ Plantilla de Backend

### **OpciÃ³n 1: Backend con n8n Webhook**

Reemplaza `api/chat.js` con:

```javascript
// api/chat.js
export default async function handler(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Solo POST permitido' });
  }

  try {
    const { message, threadId } = req.body;

    if (!message) {
      return res.status(400).json({ error: 'Mensaje requerido' });
    }

    // Configurar SSE
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache, no-transform');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no');

    // Generar o usar threadId existente
    const currentThreadId = threadId || `thread_${Date.now()}`;
    res.write(`data: ${JSON.stringify({ type: 'thread', threadId: currentThreadId })}\n\n`);
    res.flush?.();

    // Llamar a tu webhook de n8n
    const n8nWebhookURL = process.env.N8N_WEBHOOK_URL;

    const response = await fetch(n8nWebhookURL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        message: message,
        threadId: currentThreadId
      })
    });

    const data = await response.json();

    // Simular streaming del texto recibido
    const fullText = data.response || data.text || data.message;
    const chunkSize = 5; // Caracteres por chunk

    for (let i = 0; i < fullText.length; i += chunkSize) {
      const chunk = fullText.substring(i, i + chunkSize);
      res.write(`data: ${JSON.stringify({ type: 'text', content: chunk })}\n\n`);
      res.flush?.();

      // Delay para efecto de escritura (opcional)
      await new Promise(resolve => setTimeout(resolve, 20));
    }

    res.write(`data: ${JSON.stringify({ type: 'done' })}\n\n`);
    res.end();

  } catch (error) {
    console.error('Error:', error);
    if (!res.headersSent) {
      return res.status(500).json({ error: error.message });
    }
    res.write(`data: ${JSON.stringify({ type: 'error', error: error.message })}\n\n`);
    res.end();
  }
}
```

### **OpciÃ³n 2: Backend con Google Gemini API Directo**

```javascript
// api/chat.js
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

export default async function handler(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Solo POST permitido' });
  }

  try {
    const { message, threadId } = req.body;

    if (!message) {
      return res.status(400).json({ error: 'Mensaje requerido' });
    }

    // Configurar SSE
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache, no-transform');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no');

    // Thread ID
    const currentThreadId = threadId || `thread_${Date.now()}`;
    res.write(`data: ${JSON.stringify({ type: 'thread', threadId: currentThreadId })}\n\n`);
    res.flush?.();

    // Gemini streaming
    const model = genAI.getGenerativeModel({ model: 'gemini-pro' });

    const result = await model.generateContentStream(message);

    for await (const chunk of result.stream) {
      const chunkText = chunk.text();
      res.write(`data: ${JSON.stringify({ type: 'text', content: chunkText })}\n\n`);
      res.flush?.();
    }

    res.write(`data: ${JSON.stringify({ type: 'done' })}\n\n`);
    res.end();

  } catch (error) {
    console.error('Error:', error);
    if (!res.headersSent) {
      return res.status(500).json({ error: error.message });
    }
    res.write(`data: ${JSON.stringify({ type: 'error', error: error.message })}\n\n`);
    res.end();
  }
}
```

---

## ğŸ” Variables de Entorno

### **Actualizar `.env`**

SegÃºn tu backend, actualiza las variables:

```bash
# Para n8n
N8N_WEBHOOK_URL=https://tu-instancia-n8n.com/webhook/tu-webhook-id

# Para Gemini
GEMINI_API_KEY=tu-api-key-de-gemini

# Para otro servicio
CUSTOM_API_URL=https://tu-api.com
CUSTOM_API_KEY=tu-key
```

### **Configurar en Vercel**

```bash
# Agregar variables de entorno en Vercel
vercel env add N8N_WEBHOOK_URL production
vercel env add GEMINI_API_KEY production
```

---

## ğŸ“¦ Dependencias del Backend

### **Para n8n (no requiere dependencias adicionales)**
```json
{
  "dependencies": {}
}
```

### **Para Gemini**
```bash
npm install @google/generative-ai
```

Actualiza `package.json`:
```json
{
  "dependencies": {
    "@google/generative-ai": "^0.1.3"
  }
}
```

---

## ğŸ¨ Frontend (NO MODIFICAR)

El archivo `public/index.html` contiene:

âœ… **Componentes UI:**
- Pantalla de bienvenida con icono ğŸ“–
- 4 conversation starters
- Sistema de streaming en tiempo real
- Cursor animado con cruz âœï¸
- Auto-scroll optimizado
- Footer con crÃ©ditos SINODE

âœ… **Funcionalidades:**
- Efecto de escritura carÃ¡cter por carÃ¡cter (15ms delay)
- Cola de texto (TextQueue) para streaming suave
- Sistema de debugging configurable (`DEBUG_MODE`)
- Persistencia de conversaciÃ³n con `threadId`
- Responsive para mÃ³vil

**âš ï¸ IMPORTANTE:** No modifiques `public/index.html` a menos que quieras cambiar el diseÃ±o visual.

---

## ğŸ§ª Testing Local

```bash
# Iniciar servidor de desarrollo
vercel dev

# O con npm (si configuras script)
npm run dev
```

Abre: http://localhost:3000

**Verifica que:**
1. âœ… Aparece la pantalla de bienvenida
2. âœ… Los 4 conversation starters funcionan
3. âœ… El texto se muestra con efecto de escritura
4. âœ… El cursor âœï¸ aparece mientras escribe
5. âœ… Los errores se muestran correctamente

---

## ğŸš€ Deploy a ProducciÃ³n

```bash
# Deploy a Vercel
vercel --prod

# La URL serÃ¡ algo como:
# https://tu-proyecto-xxx.vercel.app
```

---

## ğŸ” Debugging del Backend

### **Activar modo DEBUG**

En `api/chat.js`, cambia:
```javascript
const DEBUG_MODE = true; // Activar logs detallados
```

En `public/index.html`, cambia (lÃ­nea ~330):
```javascript
const DEBUG_MODE = true; // Ver logs en consola del navegador
```

### **Logs esperados en consola del navegador:**

Con `DEBUG_MODE = true`:
```
Enviando mensaje con streaming a: /api/chat
Thread ID recibido: thread_123456
ğŸ”¹ [timestamp] Chunk recibido (5 chars): Hola
ğŸ”¹ [timestamp] Chunk recibido (3 chars): mun
ğŸ”¹ [timestamp] Chunk recibido (2 chars): do
Respuesta completada
```

---

## ğŸ“Š Checklist de MigraciÃ³n

- [ ] Clonar repositorio
- [ ] Cambiar remote de Git
- [ ] Instalar dependencias (`npm install`)
- [ ] Modificar `api/chat.js` con nuevo backend
- [ ] Configurar variables de entorno (`.env`)
- [ ] Probar localmente (`vercel dev`)
- [ ] Verificar streaming funciona correctamente
- [ ] Configurar variables en Vercel (`vercel env add`)
- [ ] Deploy a producciÃ³n (`vercel --prod`)
- [ ] Probar en producciÃ³n
- [ ] Desactivar `DEBUG_MODE` en producciÃ³n

---

## ğŸ†˜ Troubleshooting

### **Problema: El texto aparece todo de golpe**

**SoluciÃ³n:** AsegÃºrate de:
1. Enviar chunks pequeÃ±os (5-10 caracteres)
2. Llamar a `res.flush?.()` despuÃ©s de cada `res.write()`
3. Headers SSE correctos (especialmente `X-Accel-Buffering: no`)

### **Problema: Error de CORS**

**SoluciÃ³n:** Verifica los headers CORS en `api/chat.js`:
```javascript
res.setHeader('Access-Control-Allow-Origin', '*');
res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
```

### **Problema: ThreadId no persiste**

**SoluciÃ³n:** AsegÃºrate de enviar el threadId en el primer evento:
```javascript
res.write(`data: ${JSON.stringify({ type: 'thread', threadId: yourThreadId })}\n\n`);
```

### **Problema: Streaming no funciona en producciÃ³n**

**SoluciÃ³n:** Vercel tiene timeout de 10 segundos para Hobby plan. AsegÃºrate de:
1. Enviar chunks rÃ¡pidamente
2. No tener delays largos entre chunks
3. Considerar Vercel Pro si necesitas mÃ¡s tiempo

---

## ğŸ“š Recursos Adicionales

### **APIs Compatibles**

Este frontend funciona con cualquier backend que implemente el contrato SSE. Ejemplos:

- âœ… OpenAI API (GPT-4, GPT-3.5)
- âœ… Google Gemini API
- âœ… Anthropic Claude API
- âœ… n8n Webhooks
- âœ… Custom RAG con LangChain
- âœ… Azure OpenAI
- âœ… Ollama (local)

### **Ejemplo con LangChain**

```javascript
import { ChatOpenAI } from 'langchain/chat_models/openai';

const chat = new ChatOpenAI({
  streaming: true,
  callbacks: [{
    handleLLMNewToken(token) {
      res.write(`data: ${JSON.stringify({ type: 'text', content: token })}\n\n`);
      res.flush?.();
    }
  }]
});

await chat.call([{ role: 'user', content: message }]);
```

---

## ğŸ“ Soporte

**Creado por:** SINODE
**Web:** https://sinode.org
**AÃ±o:** 2025

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo licencia MIT. Puedes modificarlo y distribuirlo libremente.

---

## âœ¨ CrÃ©ditos

- **Frontend:** DiseÃ±o inspirado en ChatGPT Custom GPT
- **Backend Original:** OpenAI Assistant API
- **Framework:** Vercel Serverless Functions
- **Streaming:** Server-Sent Events (SSE)
- **Animaciones:** CSS Keyframes + JavaScript TextQueue

---

**Â¡Listo para clonar y modificar!** ğŸš€
